{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Support  Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I'll explain how to use quadratic programming (cvxopt) to find the solution to the dual version of the soft margin SVM optimization problem. I'll also show how to implement the Pegasos algorithm for soft margin SVMs, which uses stochastic gradient descent and a decaying step size. This write-up was inspired by homework from MIT course 6.867 (Machine Learning). \n",
    "\n",
    "The solution to the soft margin SVM can be found by solving the following optimization problem:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*} &\\min_{w, b} \\frac{1}{2} \\lVert w \\rVert^2 + C\\sum_{i}^N \\xi_i \\\\ \\text{subject to}\\; \\; \\; &y_i(w^Tx_i + b) \\geq 1 - \\xi_i,  \\; \\xi_i \\geq 0 \\; \\forall i \\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dual optimization problem is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*} \n",
    "&\\max_{\\alpha}  \\sum_{i=1}^N \\alpha_i - \\frac{1}{2}\\sum_{i=1}^N \\sum_{i'=1}^N \\alpha_i \\alpha_{i'}y_i y_{i'}x_i^T x_{i'} \\\\\n",
    "\\text{subject to} \\; \\; \\; &\\sum_{i=1}^N \\alpha_i y_i = 0, \\; 0 \\leq \\alpha_i \\leq C \\; \\forall i\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important thing to note here is that the values of $\\alpha_i$ inform us about the location of the $x_i$ with respect to the classifier. In particular, \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*} & \\alpha_i = C \\quad \\text{if} \\quad &y_if(x_i) < 1 \\\\\n",
    "& 0 < \\alpha_i < C \\quad \\text{if} &\\quad y_if(x_i) = 1 \\\\ & \\alpha_i = 0 \\quad \\text{if} \\quad &y_if(x_i) > 1\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By definition support vectors are all $x_i$ having $\\alpha_i >0$ (note that this includes all points that are misclassified as well as points that are correctly classified and on or within the margin). The conditions above tell us that the support vectors that are right on the margin are the ones satisfying $0 < \\alpha_i < C$. Points that are correctly classified with margin have $\\alpha_i = 0$.\n",
    "It is also useful to translate from the dual to primal optimization solutions. We have that $$w = \\sum_{i=1}^N \\alpha_ix_iy_i.$$ To calculate $b$ we may use any one of the support vectors $x_s$ living on the margin (i.e., having $0 < \\alpha_i <C$) since $b = 1 - w^Tx_s$ in that case. Many sources recommend taking the mean over all such support vectors during implementation for numerical stability. It is entirely possible however, that there are no points for which $0 < \\alpha_i <C$. In this case we may find $b$ by examining the initial optimization problem. This boils down to solving $$\\text{argmin}_b \\sum_{s}\\max(0, 1- y_s(w^Tx_s + b))$$ where the sum is taken over all support vectors. (This is obtained from the original optimization problem by discarding the terms coming from points that are correctly classified with margin and therefore have $\\max(0, 1- y_s(w^Tx_s + b)) = 0$, as well as discarding the regularization term which has been optimized. Since for a support vector $1- y_s(w^Tx_s + b)\\geq 0$, the above optimization problem is equivalent to $$\\text{argmin}_b \\sum_{s}(1- y_s(w^Tx_s + b)) = \\text{argmin}_b \\sum_s (y_s - w^Tx_s - b)$$ whose solution is the median of the $y_s - w^Tx_s$ values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in order to use cvxopt to solve the dual optimization problem, we must transform it into the form:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "&\\min_x \\frac{1}{2}x^TPx + q^Tx \\\\\n",
    "\\text{subject to} \\quad & Gx < h, Ax = b\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this we will consider instead the minimization problem:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\min_{\\alpha} \\frac{1}{2}\\sum_{i=1}^N \\sum_{i'=1}^N \\alpha_i \\alpha_{i'}y_i y_{i'}x_i^T x_{i'} -\\sum_{i=1}^N \\alpha_i.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from cvxopt import matrix, solvers\n",
    "import statistics \n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = np.dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 2\n",
    "def gaussian_kernel(x1, x2):\n",
    "    res = math.e**(-gamma*np.dot(x1-x2,x1-x2))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(X, kernel):\n",
    "    num_samples = X.shape[0]\n",
    "    K = np.zeros((num_samples, num_samples))\n",
    "    for i in range(num_samples):\n",
    "        for j in range(num_samples):\n",
    "            K[i, j] = kernel(X[i], X[j])\n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_svm_solver(X, Y, C, kernel):\n",
    "    num_samples, num_features = X.shape\n",
    "    K = gram_matrix(X, kernel)\n",
    "    P = matrix(np.outer(Y,Y)*K)\n",
    "    q = matrix(np.ones(num_samples)*-1)\n",
    "    A = matrix(Y, (1, num_samples))\n",
    "    b = matrix(0.0)\n",
    "    G = matrix(np.vstack((np.diag(np.ones(num_samples) * -1), np.identity(num_samples))))\n",
    "    h = matrix(np.hstack((np.zeros(num_samples), np.ones(num_samples) * C)))\n",
    "    solution = solvers.qp(P, q, G, h, A, b)\n",
    "    alpha = np.array(solution['x'])\n",
    "    threshold_alpha = np.copy(alpha)\n",
    "    sv = alpha < 1e-4\n",
    "    threshold_alpha[sv] = 0\n",
    "    return threshold_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -2.2102e+02 -1.1032e+03  6e+03  3e+00  2e-14\n",
      " 1: -1.4926e+02 -7.6319e+02  1e+03  4e-01  1e-14\n",
      " 2: -1.3704e+02 -2.4046e+02  1e+02  1e-02  1e-14\n",
      " 3: -1.5807e+02 -1.8764e+02  3e+01  3e-03  9e-15\n",
      " 4: -1.6226e+02 -1.8229e+02  2e+01  2e-03  8e-15\n",
      " 5: -1.6618e+02 -1.7648e+02  1e+01  7e-04  8e-15\n",
      " 6: -1.6783e+02 -1.7420e+02  6e+00  3e-04  8e-15\n",
      " 7: -1.6882e+02 -1.7293e+02  4e+00  2e-04  8e-15\n",
      " 8: -1.6948e+02 -1.7191e+02  2e+00  7e-05  8e-15\n",
      " 9: -1.6966e+02 -1.7168e+02  2e+00  4e-05  8e-15\n",
      "10: -1.7047e+02 -1.7073e+02  3e-01  3e-06  1e-14\n",
      "11: -1.7057e+02 -1.7062e+02  6e-02  5e-07  1e-14\n",
      "12: -1.7059e+02 -1.7059e+02  2e-03  9e-09  1e-14\n",
      "13: -1.7059e+02 -1.7059e+02  2e-05  9e-11  9e-15\n",
      "Optimal solution found.\n"
     ]
    }
   ],
   "source": [
    "train = np.loadtxt('data/data2_train.csv')\n",
    "X = train[:,0:2]\n",
    "Y = train[:, 2:3]\n",
    "C = 1\n",
    "num_samples, num_features = X.shape\n",
    "a = soft_svm_solver(X, Y, C, kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01007478]\n"
     ]
    }
   ],
   "source": [
    "# we now compute the bias term as described above\n",
    "bias_list = [] # this is a list of values y_s - w^T x_s for all support vectors\n",
    "for idx in np.nonzero(a)[0]:\n",
    "    b_sum = 0\n",
    "    for i in range(X.shape[0]):\n",
    "        b_sum += a[i]*Y[i]*kernel(X[i], X[idx])\n",
    "    bias_list.append(Y[idx] - b_sum)\n",
    "b = statistics.median(bias_list)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To predict the classification of a new point, we use the following formula \n",
    "$$\n",
    "f(x) = w^Tx + b = b + \\sum_{i=1}^N \\alpha_iy_iK(x_i, x).\n",
    "$$ \n",
    "When using linear kernels it's faster to initially compute the weight vector $w$ and then predict using the first formula, but this doesn't work when trying to use nonlinear kernels since then we have to first compute the transformed feature vectors. The following function finds $w$ in the case when the kernel is the standard dot product. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.31477194 -0.04219494]\n"
     ]
    }
   ],
   "source": [
    "def find_w():\n",
    "    coef = a*Y\n",
    "    coef = coef.flatten()\n",
    "    w = np.sum(X*coef[:, np.newaxis], axis=0)\n",
    "    return w\n",
    "print(find_w())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "x = np.linspace(-5,5,100)\n",
    "ax.set(xlim=(-5, 5), ylim=(-5, 5))\n",
    "ax.scatter(train[np.ix_(train[:,2]>0, [0])], train[np.ix_(train[:,2]>0, [1])], color = 'g')\n",
    "ax.scatter(train[np.ix_(train[:,2]<0, [0])], train[np.ix_(train[:,2]<0, [1])], color = 'r')\n",
    "plt.plot(x,(-find_w()[0]*x - b)/find_w()[1], '-b')\n",
    "plt.plot(x,(-find_w()[0]*x - b +1)/find_w()[1], ':b')\n",
    "plt.plot(x,(-find_w()[0]*x - b -1)/find_w()[1], ':b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.73223246]\n"
     ]
    }
   ],
   "source": [
    "def predictSVM(x):\n",
    "    pred_sum = 0\n",
    "    for i in range(num_samples):\n",
    "        pred_sum += a[i]*Y[i]*kernel(X[i],x)\n",
    "    return pred_sum + b\n",
    "print(predictSVM(X[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1725\n"
     ]
    }
   ],
   "source": [
    "def classification_error_rate(X, Y, func):\n",
    "    errors = 0\n",
    "    for i in range(X.shape[0]):\n",
    "        if np.sign(func(X[i])) != np.sign(Y[i]):\n",
    "            errors +=1\n",
    "    return errors/X.shape[0]\n",
    "print(classification_error_rate(X, Y, predictSVM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_validation, Y_validation) = csv_to_arrays()\n",
    "print(classification_error_rate(X_validation, Y_validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pegasos algorithm attempts to find the solution to the soft SVM problem by using stochastic gradient descent and a decaying step size. It attempts to find the solution to the optimization problem: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\min_w \\frac{\\lambda}{2}\\lVert w \\rVert^2 + \\frac{1}{N}\\sum_{i=1}^N \\max(0, 1-y_i(w^Tx_i + b).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find the bias term $b$ by creating an outer loop and evaluating various possible choices for $b$ after having optimized $w$. The reason for this is that the original Pegasos paper recommends this approach due to its faster convergence. The step size is originally set to $\\frac{1}{\\lambda}$ and decays with $\\frac{1}{t}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for pegasos_linearSVM, finds value of the above objective function\n",
    "def find_loss(X, Y, w, b, lam):\n",
    "    loss = (lam/2)*np.dot(w, w)\n",
    "    for i in range(num_samples):\n",
    "        loss += (1/num_samples)*np.maximum(0, 1-Y[i]*(np.dot(w, X[i] + b)))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pegasos_linearSVM(X, Y, lam, max_epochs):\n",
    "    best_loss = math.inf\n",
    "    epoch = 0\n",
    "    t = 0\n",
    "    num_samples, num_features = X.shape\n",
    "    w = np.zeros(num_features)\n",
    "    b_seq = np.linspace(-50, 50, 500)\n",
    "    for b in b_seq:\n",
    "        while epoch < max_epochs:\n",
    "            epoch += 1\n",
    "            for i in range(num_samples):\n",
    "                t += 1\n",
    "                eta_t = 1/(t*lam)\n",
    "                if Y[i]*(np.dot(w, X[i])+b) < 1:\n",
    "                    w = (1-eta_t*lam)*w + eta_t*X[i]*Y[i]\n",
    "                else:\n",
    "                    w = (1- eta_t*lam)*w\n",
    "        if find_loss(X, Y, w, b, lam) < best_loss:\n",
    "            best_loss = find_loss(X, Y, w, b, lam)\n",
    "            best_pair = (w,b)\n",
    "    return best_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([2.17566199, 0.13991093]), 0.10020040080160442)\n"
     ]
    }
   ],
   "source": [
    "(w, b) = pegasos_linearSVM(X, Y, .2, 100)\n",
    "print((w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7301844906263515\n",
      "0.1725\n"
     ]
    }
   ],
   "source": [
    "def predict_linearSVM(x):\n",
    "    return np.dot(w, x) + b\n",
    "print(predict_linearSVM(X[4]))\n",
    "print(classification_error_rate(X, Y, predict_linearSVM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also implement Pegasos with a Gaussian RBF kernel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pegasos_gaussianSVM(X, Y, K, lam, max_epochs):\n",
    "    t = 0\n",
    "    num_samples = X.shape[0]\n",
    "    alpha = np.zeros((num_samples))\n",
    "    epoch = 0\n",
    "    while epoch < max_epochs:\n",
    "        epoch += 1\n",
    "        for i in range(num_samples):\n",
    "            t += 1\n",
    "            eta_t = 1/(t*lam)\n",
    "            test_sum =0 \n",
    "            for j in range(num_samples):\n",
    "                test_sum += alpha[j]*Y[j]*K[i, j]\n",
    "            if Y[i]*test_sum < 1:\n",
    "                alpha[i] = (1-eta_t*lam)*alpha[i] + eta_t\n",
    "            else:\n",
    "                alpha[i] = (1-eta_t*lam)*alpha[i]\n",
    "    return alpha\n",
    "\n",
    "alpha = pegasos_gaussianSVM(X, Y, gram_matrix(X, gaussian_kernel), .02, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24.38508962]\n",
      "0.06\n"
     ]
    }
   ],
   "source": [
    "def predict_gaussianSVM(x):\n",
    "    pred_sum = 0\n",
    "    for i in range(num_samples):\n",
    "        pred_sum += alpha[i]*Y[i]*gaussian_kernel(X[i], x)\n",
    "    return pred_sum\n",
    "\n",
    "print(predict_gaussianSVM(X[4]))\n",
    "print(classification_error_rate(X, Y, predict_gaussianSVM))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
