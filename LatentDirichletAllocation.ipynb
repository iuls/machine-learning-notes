{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation with Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I'll use Scikit-Learn to perform Latent Dirichlet Allocation on the 20 Newsgroups dataset. The implementation in Scikit-Learn uses the online variational Bayes algorithm. I'll also explain the Gibbs sampling approach and present an example that illustrates intuitively why it works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import stop_words, text\n",
    "from pprint import pprint\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fetch_20newsgroups function will by default shuffle the data and import just the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_data = fetch_20newsgroups(random_state = 1, remove = ('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism',\n",
      " 'comp.graphics',\n",
      " 'comp.os.ms-windows.misc',\n",
      " 'comp.sys.ibm.pc.hardware',\n",
      " 'comp.sys.mac.hardware',\n",
      " 'comp.windows.x',\n",
      " 'misc.forsale',\n",
      " 'rec.autos',\n",
      " 'rec.motorcycles',\n",
      " 'rec.sport.baseball',\n",
      " 'rec.sport.hockey',\n",
      " 'sci.crypt',\n",
      " 'sci.electronics',\n",
      " 'sci.med',\n",
      " 'sci.space',\n",
      " 'soc.religion.christian',\n",
      " 'talk.politics.guns',\n",
      " 'talk.politics.mideast',\n",
      " 'talk.politics.misc',\n",
      " 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "pprint(list(newsgroups_data.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at some of the documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Well i'm not sure about the story nad it did seem biased. What\\nI disagree with is your statement that the U.S. Media is out to\\nruin Israels reputation. That is rediculous. The U.S. media is\\nthe most pro-israeli media in the world. Having lived in Europe\\nI realize that incidences such as the one described in the\\nletter have occured. The U.S. media as a whole seem to try to\\nignore them. The U.S. is subsidizing Israels existance and the\\nEuropeans are not (at least not to the same degree). So I think\\nthat might be a reason they report more clearly on the\\natrocities.\\n\\tWhat is a shame is that in Austria, daily reports of\\nthe inhuman acts commited by Israeli soldiers and the blessing\\nreceived from the Government makes some of the Holocaust guilt\\ngo away. After all, look how the Jews are treating other races\\nwhen they got power. It is unfortunate.\\n\",\n",
       " \"\\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to read the FAQ, etc. and actually accept hard\\natheism?  No, you need a little leap of faith, Jimmy.  Your logic runs out\\nof steam!\\n\\n\\n\\n\\n\\n\\n\\nJim,\\n\\nSorry I can't pity you, Jim.  And I'm sorry that you have these feelings of\\ndenial about the faith you need to get by.  Oh well, just pretend that it will\\nall end happily ever after anyway.  Maybe if you start a new newsgroup,\\nalt.atheist.hard, you won't be bummin' so much?\\n\\n\\n\\n\\n\\n\\nBye-Bye, Big Jim.  Don't forget your Flintstone's Chewables!  :) \\n--\\nBake Timmons, III\"]"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_data.data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([17,  0, 17, 11, 10, 15,  4, 17, 13, 12])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_data.target[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 480,\n",
       " 1: 584,\n",
       " 2: 591,\n",
       " 3: 590,\n",
       " 4: 578,\n",
       " 5: 593,\n",
       " 6: 585,\n",
       " 7: 594,\n",
       " 8: 598,\n",
       " 9: 597,\n",
       " 10: 600,\n",
       " 11: 595,\n",
       " 12: 591,\n",
       " 13: 594,\n",
       " 14: 593,\n",
       " 15: 599,\n",
       " 16: 546,\n",
       " 17: 564,\n",
       " 18: 465,\n",
       " 19: 377}"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique, counts = np.unique(newsgroups_data.target, return_counts = True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object CountVectorizer combined with the method fit_transform will take in our documents and output a matrix of size (number of documents) x (vocabulary size) where the $i,j$ entry is the number of times word $j$ appears in document $i$. The size of the vocabulary will depend on whether or not we decide to use stop words and what the values of max_df and min_df are. Setting stop_words = \"english\" as an input to the CountVectorizer object will tell it to ignore a certain fixed list of common English words. The options max_df and min_df are either integers or floats between 0 and 1.0: if a word is found in greater than max_df documents, we ignore it; similarly, if a word is found in less than min_df documents, we ignore it. By setting appropriate values of max_df and min_df, we allow CountVectorizer to find the common words in our specific corpus. We'll try both options. It's also good to note that CountVectorizer by default sets the token pattern to \\b\\w\\w+\\b, from which we can see that it ignores punctuation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stop_words = text.ENGLISH_STOP_WORDS.union([\"like\", \"does\", \"going\", \"said\", \"say\", \"did\", \"use\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words = my_stop_words, max_df =.3, min_df = 3)\n",
    "tf = vectorizer.fit_transform(newsgroups_data.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 26740)"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Latent Dirichlet Allocation we must provide a number of topics (n_components) for the procedure to learn. We'll choose the online variational Bayes method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=20, max_iter=10, learning_method='online', random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "                          evaluate_every=-1, learning_decay=0.7,\n",
       "                          learning_method='online', learning_offset=10.0,\n",
       "                          max_doc_update_iter=100, max_iter=10,\n",
       "                          mean_change_tol=0.001, n_components=20, n_jobs=None,\n",
       "                          perp_tol=0.1, random_state=0, topic_word_prior=None,\n",
       "                          total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 : armenian armenians turkish people went turkey came armenia turks didn\n",
      "Topic 1 : team year game games play season hockey league players win\n",
      "Topic 2 : don just know think people time good way ve want\n",
      "Topic 3 : window output widget entry motif program file server mouse set\n",
      "Topic 4 : fpu hr uiuc 147 200 processor vt 250 500 hst\n",
      "Topic 5 : better left points nazi bad run fans men definitely time\n",
      "Topic 6 : edu file available information mail files com ftp program software\n",
      "Topic 7 : key used new car good encryption need using chip high\n",
      "Topic 8 : max edu com 34 9v au mk gov cs nist\n",
      "Topic 9 : god jesus christian bible church believe christians faith christ life\n",
      "Topic 10 : space israel war jews nasa israeli world earth launch 000\n",
      "Topic 11 : bike 000 edu new university health medical 1993 research pain\n",
      "Topic 12 : pl ms uw tm mr mw mp ww mm q6\n",
      "Topic 13 : cx w7 mv chz t7 bh lk hz ck c_\n",
      "Topic 14 : greek water jim weapon frank street greece dept western william\n",
      "Topic 15 : drive card bit disk windows scsi memory dos mac pc\n",
      "Topic 16 : government people law gun mr president state public rights states\n",
      "Topic 17 : religious religion exist science atheism atheists alt moral islam objective\n",
      "Topic 18 : ax b8f g9v a86 145 1d9 0t 1t 34u 3t\n",
      "Topic 19 : 00 10 25 11 15 12 db 20 14 16\n"
     ]
    }
   ],
   "source": [
    "tf_feature_names = vectorizer.get_feature_names() # gets tokens found by CountVectorizer (the columns of tf)\n",
    "# we'll print out the 10 top words per topic\n",
    "for index, topic in enumerate(lda.components_):\n",
    "    top_words = \" \".join([tf_feature_names[i] for i in topic.argsort()[: -11:-1]])\n",
    "    print(\"Topic\", index, \":\" , top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transform method for lda will return a matrix of size (number of documents) x (n_components) which reflects the document-topic distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 20)\n",
      "[0.00081967 0.00081967 0.25336552 0.00081967 0.00081967 0.00081967\n",
      " 0.00081967 0.00081967 0.00081967 0.00081967 0.58427815 0.00081967\n",
      " 0.00081967 0.00081967 0.00081967 0.00081967 0.14842191 0.00081967\n",
      " 0.00081967 0.00081967]\n"
     ]
    }
   ],
   "source": [
    "tf_transform = lda.transform(tf)\n",
    "print(tf_transform.shape)\n",
    "print(tf_transform[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3135.236527202665"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.perplexity(tf) # perhaps not a great metric for evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The components_ method outputs a matrix of size (n_components) x (number of tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 26740)"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_topic = np.argmax(tf_transform, axis=1) # selects the most likely topic per document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10,  2, 10,  7, 11,  2, 12,  2,  2,  7,  6,  7, 11,  5,  2,  2, 16,\n",
       "        8,  4,  2,  7,  5, 16,  5,  1,  9,  7,  7, 19,  2,  7,  2,  7,  9,\n",
       "       10, 17,  2, 17,  2,  2, 15, 11,  7,  6, 12,  6,  6,  7,  2, 15,  7,\n",
       "        0, 17,  7,  7, 10,  7,  2,  7, 16,  7,  2, 14,  2,  7,  7,  7,  5,\n",
       "       17, 16, 11, 11, 15,  2, 15,  7,  2,  7, 14,  2, 16,  2,  7,  7, 15,\n",
       "        2,  7, 11,  2,  0, 15, 10, 15,  2,  2,  9,  0,  7, 11,  7])"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_topic[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 421,\n",
       " 1: 460,\n",
       " 2: 3602,\n",
       " 3: 154,\n",
       " 4: 30,\n",
       " 5: 182,\n",
       " 6: 1176,\n",
       " 7: 1762,\n",
       " 8: 61,\n",
       " 9: 446,\n",
       " 10: 455,\n",
       " 11: 349,\n",
       " 12: 17,\n",
       " 13: 27,\n",
       " 14: 43,\n",
       " 15: 1324,\n",
       " 16: 638,\n",
       " 17: 36,\n",
       " 18: 12,\n",
       " 19: 119}"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique, counts = np.unique(max_topic, return_counts = True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The topic categories are very unbalanced - the topic having top words that are very generic (model topic number 2) is being chosen as the most likely topic for a disproportionate number of documents. Let's take original topic number 17 (mideast) and see what topics from above the model attributed to those documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 10, 2, 10, 2, 2, 2, 0, 16, 10]"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_max_topics(topic_number):\n",
    "    indices = [i for i in range(len(newsgroups_data.target)) if newsgroups_data.target[i] == topic_number]\n",
    "    max_topics = [row.argmax() for row in tf_transform[indices]]\n",
    "    return max_topics\n",
    "\n",
    "get_max_topics(17)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 76,\n",
       " 1: 4,\n",
       " 2: 197,\n",
       " 3: 2,\n",
       " 5: 2,\n",
       " 6: 5,\n",
       " 7: 5,\n",
       " 9: 9,\n",
       " 10: 206,\n",
       " 11: 4,\n",
       " 12: 1,\n",
       " 14: 6,\n",
       " 16: 43,\n",
       " 17: 1,\n",
       " 19: 3}"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique, counts = np.unique(get_max_topics(17), return_counts = True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This distribution could make some sense - excluding topic 2 (which is very generic), the other topics that get confused with topic 17 (mideast) have very related words. There are likely many other things you could do to improve the results of this model, but we'll stop here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll now explain the Gibbs sampling approach for the assignment of words to topics, following the notation from here: https://people.cs.umass.edu/~wallach/courses/s11/cmpsci791ss/readings/griffiths02gibbs.pdf. Suppose we have the following set up:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\theta^{(d_i)} &\\sim Dir(\\alpha) \\\\\n",
    "z_{i}|\\theta^{(d_i)} &\\sim Mult(\\theta^{(d_i)})\\\\\n",
    "\\phi^{(z_{i})} &\\sim Dir(\\beta) \\\\\n",
    "w_{i}|z_{i}, \\phi^{(z_{i})} &\\sim Mult(\\phi^{(z_{i})})\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where the topic for word $i$ in document $d_i$, $z_{i}$, is drawn from a multinomial distribution parametrized by $\\theta^{(d_i)}$ and $\\theta^{(d_i)}$ has a Dirichlet prior parametrized by $\\alpha$; similarly, given a topic $z_{i}$, word $w_{i}$ is drawn from a multinomial distribution parametrized by $\\phi^{(z_{i})}$ where $\\phi^{(z_{i})}$ has a Dirichlet prior parametrized by $\\beta$. Let $\\mathbf{w}$ be our vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our input will be a matrix of size (number of documents) x (size of vocabulary) (call this the DW matrix) where the $i, j$ entry tells you the number of times word $j$ appears in document $i$. In Gibbs sampling, we first initialize randomly by assigning each word in each document to a topic. This gives us a word-topic matrix WT where the $i, j$ entry tells you how many times word $i$ was assigned to topic $j$. Finally, we have a document-topic matrix DT where the $i, j$ entry tells you the number of words in topic $j$ in document $i$. We then go through each document and each word and reassign it's topic according to the relative probabilities below. A careful computation will show that:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ P(z_i = j|z_{-i}, \\mathbf{w}) \\propto \\frac{n^{(w_i)}_{-i,j} + \\beta}{n_{-i,j}^{(\\cdot)} + W\\beta} \\cdot \\frac{n^{(d_i)}_{-i,j} + \\alpha}{n_{-i, \\cdot}^{(d_i)} + T\\alpha} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $z_{-i}$ is an assignment of topics for all words except $w_i$, $W$ is the size of the vocabulary, $T$ is the number of topics, $n^{(w_i)}_{-i,j}$ is the total number of times word $w_i$ is assigned to topic $j$, $n_{-i,j}^{(\\cdot)}$ is the total number of words assigned to topic $j$, $n^{(d_i)}_{-i,j}$ is the number of words from document $d_i$ assigned to topic $j$, and finally $n_{-i, \\cdot}^{(d_i)}$ is the total number of words in document $d_i$ (and we enforce that all of these counts exclude the current word). For a derivation of this formula, see the pdf linked to above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get some intuition for why this procedure reaches a stable distribution by considering a very simple example. Suppose you have two documents $d_1$ and $d_2$. Now suppose that $d_1$ is \"a a a a a\" and $d_2$ is \"b b b b b\". Suppose we have two topics $t_1$ and $t_2$ and we randomly initialize by labelling the first three words of both documents by $t_1$ and the latter two words of both documents by $t_2$. In this degenerate case we can ignore the Dirichlet priors (indeed, by taking the limit as $\\beta, \\alpha \\rightarrow 0$ we obtain two binomial priors). The above probability will become $ P(z_i = j|z_{-i}, \\mathbf{w}) \\propto \\frac{n^{(w_i)}_{-i,j}}{n_{-i,j}^{(\\cdot)}} \\cdot \\frac{n^{(d_i)}_{-i,j}}{n_{-i, \\cdot}^{(d_i)}}$. We would hope that ultimately all the words \"a\" are assigned to one topic with high probability and likewise for word \"b\" and the other topic. In order to see the pattern here faster, let's assume that at each update step we select the topic with the higher relative probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word 1 document 1: $P(\\text{topic} = t_1) \\propto \\frac{2}{5} \\cdot \\frac{2}{4} = \\frac{1}{5}$ and $P(\\text{topic} = t_2) \\propto \\frac{2}{4} \\cdot \\frac{2}{4} = \\frac{1}{4}$. So, word 1 in document 1 is now assigned to $t_2$. The first factor is the probability of the word given the topic, so it considers how common the topic assignment is in the other documents, while the second term is the probability of the topic given the document, so it considers how important the topic is in the document at hand.\n",
    "\n",
    "word 2 document 1: $P(\\text{topic} = t_1) \\propto \\frac{1}{4} \\cdot \\frac{1}{4} = \\frac{1}{16}$ and $P(\\text{topic} = t_2) \\propto \\frac{3}{5} \\cdot \\frac{3}{4} = \\frac{9}{20}$. So, word 2 in document 1 is now assigned to $t_2$.\n",
    "\n",
    "And in this way, we can continue and be convinced that the words will be assigned to topics in a coherent manner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
